{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Solar API로 번역하기**\n",
    "> **SamSum 데이터셋의 라이센스는 cc-by-nc-nd-4.0로 변경 금지 데이터임을 뒤늦게 인지하여 수정하였습니다. 변경 가능한 데이터셋을 이용해주세요.**  \n",
    "> **해당 데이터를 학습 데이터 활용하여 오히려 점수가 떨어진 점 참고바랍니다.**  \n",
    "> ~~아래 코드는 **Upstage AI Lab**의 일상 대화 요약 대회에서 **Solar API를 활용**해 **[SamSum](https://huggingface.co/datasets/samsum)** 데이터셋을 번역했던 코드입니다.~~   \n",
    "> ~~**DialogSum** 데이터셋과 유사한 **SamSum** 데이터셋을 번역하였으며 **DialogSum 데이터셋과 최대한 비슷해 지도록** 전처리 과정을 포함하고있습니다.~~"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import json\n",
    "import glob\n",
    "\n",
    "import random\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 Data Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder_path = \"../data\"\n",
    "\n",
    "with open(os.path.join(folder_path, \"train.json\"), \"r\", encoding='UTF8') as f:\n",
    "    train_data = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.DataFrame(train_data)\n",
    "df_train = df_train[df_train['dialogue'] != \"\"].reset_index(drop=True)\n",
    "\n",
    "del train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 Data Process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 정규식 패턴에 해당하는 문자열을 replace하는 함수\n",
    "def remove_extra_spc(x, pattern, replace_text):\n",
    "    return re.sub(pattern, replace_text, x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 이모지 및 특수문자 확인\n",
    "pattern = r\"[^a-zA-Z0-9\\s!$%&*_+-=~'\\\"\\|:\\\\.,/?]\"\n",
    "sp_list = []\n",
    "for data in df_train['dialogue']:\n",
    "    sp_list += re.findall(pattern, data)\n",
    "    \n",
    "    sp_list = list(set(sp_list))\n",
    "    \n",
    "sp_list, len(sp_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 개행문자 기준으로 대화를 나눴을 때 비어있는 리스트 제거\n",
    "def remove_empty(x):\n",
    "    text_list = x.copy()\n",
    "    for i in range(len(x)):\n",
    "        if x[i] == \"\":\n",
    "            print(\"empty!\")\n",
    "            text_list.pop(i)\n",
    "            \n",
    "    return text_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 마침표가 없는 문장에 마침표 추가\n",
    "def add_fullstop(x):\n",
    "    pattern = r\"[^!?.]$\"\n",
    "    match = re.search(pattern, x)\n",
    "    if match:\n",
    "        x+=\".\"\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 한사람이 연속적으로 발화하는 경우 해당 문장을 합쳐서 반환(원본 학습데이터와 비슷하게 맞추기 위함)\n",
    "def continuous_talking_paltten(data):\n",
    "    speak_list = []\n",
    "    stack_text = data[0]\n",
    "\n",
    "    pattern = '[a-zA-Z\\'\\s\\-,\\._]+:'\n",
    "    match_now = re.match(pattern, data[0])\n",
    "    # now_person = match_now.group(0)\n",
    "    \n",
    "    try:\n",
    "        now_person = match_now.group(0)\n",
    "    except Exception as e:\n",
    "        # print(e)\n",
    "        # print(data)\n",
    "        stack_text += \"#Error_Text#\"\n",
    "\n",
    "    for i in range(1, len(data)):    \n",
    "        match_now = re.match(pattern, data[i])\n",
    "        \n",
    "        try:\n",
    "            if now_person == match_now.group(0):\n",
    "                now_text = re.sub(pattern, \"\", data[i], count=1)\n",
    "                stack_text = stack_text + \" \" + now_text\n",
    "                \n",
    "                if i == len(data)-1:\n",
    "                    speak_list.append(stack_text)\n",
    "            else:\n",
    "                now_person = match_now.group(0)\n",
    "                speak_list.append(stack_text)\n",
    "                stack_text = data[i]\n",
    "                \n",
    "                if i == len(data)-1:\n",
    "                    speak_list.append(stack_text)\n",
    "        except Exception as e:\n",
    "            # print(e)\n",
    "            # print(data)\n",
    "            speak_list.append(\"#Error_Text#\" + data[i])\n",
    "            \n",
    "    \n",
    "    return speak_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 대화 순서 저장(확인용)\n",
    "def get_talking_sequence(x):\n",
    "    pattern = '[a-zA-Z\\'\\s\\-,\\._가-힣]+:'\n",
    "    talking_sequence = []\n",
    "    for talk in x:\n",
    "        person = re.search(pattern, talk).group(0)\n",
    "        person = person[:-1]\n",
    "        talking_sequence.append(person)\n",
    "        \n",
    "    return talking_sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 특정 특수문자 제거\n",
    "pattern = r\"[^a-zA-Z0-9\\s!$%&*_+-=~'\\\"\\|:\\\\.,/?]\"\n",
    "df_train['dialogue'] = df_train['dialogue'].apply(lambda x: remove_extra_spc(x, pattern, \"\"))\n",
    "\n",
    "# 개행문자를 제외한 공백문자 제거\n",
    "df_train['dialogue'] = df_train['dialogue'].apply(lambda x: re.sub(r\"[\\r\\t]\", '', x))\n",
    "\n",
    "# 발화별 리스트화\n",
    "df_train['dialogue_list'] = df_train['dialogue'].apply(lambda x: x.split(\"\\n\"))\n",
    "\n",
    "# 빈 리스트 제거\n",
    "df_train['dialogue_list'] = df_train['dialogue_list'].apply(remove_empty)\n",
    "\n",
    "# 발화별 좌우공백 제거\n",
    "df_train['dialogue_list'] = df_train['dialogue_list'].apply(lambda x: [item.strip() for item in x])\n",
    "\n",
    "# 마침표 찍기\n",
    "df_train['dialogue_list'] = df_train['dialogue_list'].apply(lambda x: [add_fullstop(item) for item in x])\n",
    "\n",
    "# 연속된 발화자 이어붙히기\n",
    "df_train['dialogue_list'] = df_train['dialogue_list'].apply(continuous_talking_paltten)\n",
    "\n",
    "# 예외 데이터 삭제\n",
    "df_train['dialogue2'] = df_train['dialogue_list'].apply(lambda x: \"\\n\".join(x))\n",
    "df_train = df_train[~df_train['dialogue2'].str.contains('#Error_Text#')].reset_index(drop=True)\n",
    "\n",
    "# 발화자 순서 저장하기\n",
    "df_train['talking_sequence'] = df_train['dialogue_list'].apply(get_talking_sequence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 Solar API"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 필요한 라이브러리 설치 및 Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install httpx==0.23.2\n",
    "# !pip install openai==1.2.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 API 호출 함수 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"solar_api.txt\", \"r\") as f:\n",
    "    solar_api = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate_enko(text_en, temperature=0, print_result=False):\n",
    "    client = OpenAI(\n",
    "        api_key=solar_api, # <=== [API키 입력]\n",
    "        base_url=\"https://api.upstage.ai/v1/solar\"\n",
    "    )\n",
    "\n",
    "    stream = client.chat.completions.create(\n",
    "        model=\"solar-1-mini-translate-enko\", # 번역 모델 사용\n",
    "        messages=[\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": text_en  # 번역할 텍스트 전달\n",
    "        }\n",
    "        ],\n",
    "        temperature=temperature,\n",
    "        stream=True,\n",
    "    )\n",
    "\n",
    "    trans_str = []\n",
    "    for chunk in stream:\n",
    "        test_var = chunk\n",
    "        if chunk.choices[0].delta.content is not None:\n",
    "            trans_str.append(chunk.choices[0].delta.content)\n",
    "            # print(chunk.choices[0].delta.content, end=\"\")\n",
    "    \n",
    "    # 출력값 반환\n",
    "    trans_str = \"\".join(trans_str)\n",
    "    # print(trans_str)\n",
    "    \n",
    "    if print_result:\n",
    "        print(f\"### Source_text\\n{text_en} \\n\\n### Target_text\\n{trans_str}\")\n",
    "    \n",
    "    return trans_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.loc[10, 'dialogue2']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "translate_enko(df_train.loc[10, 'dialogue2'], print_result=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 API 호출 자동화 및 저장"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bundle_num = 5  # 파일을 저장할 단위(중간에 오류가 나거나 커널이 종료되면 비용만 나가고 데이터가 소멸될 것 대비)\n",
    "data_point = 10900 // bundle_num    # 시작점(오류 등의 이유로 중지될 경우 다시 시작할 위치)\n",
    "\n",
    "for i in range(data_point, df_train.shape[0] // bundle_num + 1):\n",
    "    print(i*bundle_num, i*bundle_num+bundle_num-1)\n",
    "    start = i*bundle_num\n",
    "    end = i*bundle_num + bundle_num-1\n",
    "    \n",
    "    trans_df = pd.DataFrame(columns=['id', 'ko_summary', 'ko_dialogue'])\n",
    "    \n",
    "    temp_df = df_train.loc[start:end].copy()\n",
    "    temp_df['ko_dialogue'] = temp_df['dialogue2'].apply(translate_enko)     # apply 함수를 활용해 번역\n",
    "    temp_df['ko_summary'] = temp_df['summary'].apply(translate_enko)\n",
    "    \n",
    "    trans_df = temp_df[['id', 'ko_summary', 'ko_dialogue']]\n",
    "    \n",
    "    # 번역후 파일 저장\n",
    "    trans_df.to_csv(f\"../data/ko_data/train{start}.csv\", index=False)\n",
    "    \n",
    "    # Solar API가 Beta 버전이라 그런지 한번에 너무 많은 호출을 하면 Too many request로 오류를 반환하기 때문에 1분을 기다려줌\n",
    "    print(\"time sleep\")\n",
    "    time.sleep(60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4 번역한 데이터 합치기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 폴더 내 모든 csv 파일 경로 가져오기\n",
    "def get_csv_files(folder_path):\n",
    "    files = glob.glob(os.path.join(folder_path, '*.csv'))\n",
    "    return files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 폴더 내의 모든 csv 파일 불러와 concat\n",
    "csv_list= get_csv_files(\"../data/ko_data\")\n",
    "new_train = pd.DataFrame(columns=['id', 'ko_summary', 'ko_dialogue'])\n",
    "for csv in csv_list:\n",
    "    temp_df = pd.read_csv(csv)\n",
    "    new_train = pd.concat([new_train, temp_df], axis=0)\n",
    "\n",
    "# 원본 데이터셋과 인덱스 순서 맞춰주기\n",
    "new_train.dropna(inplace=True)\n",
    "new_train['id'] = new_train['id'].astype(str)\n",
    "new_train['id'] = new_train['id'].str.replace(\".0\", \"\")\n",
    "\n",
    "df_train['id'] = df_train['id'].astype(str)\n",
    "df_train['index'] = df_train.index\n",
    "temp_df = df_train[['id', 'index', 'dialogue2', 'summary', 'talking_sequence']]\n",
    "\n",
    "new_train = new_train.merge(temp_df, how='left', on='id')\n",
    "new_train = new_train.sort_values(by='index').reset_index(drop=True)\n",
    "del new_train['index']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_train.head(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4 Translated Data Process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_train['ko_dialogue'] = new_train['ko_dialogue'].str.strip()\n",
    "new_train['ko_dialogue_list'] = new_train['ko_dialogue'].str.split(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 번역된 요약문 중 구어체로 번역된 요약문 제거"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ko_summary_list = new_train['ko_summary'].tolist()\n",
    "matched_idxs = []\n",
    "pattern = r\"입니다\\.$|입니다$|습니다\\.$|습니다$|니다\\.$|니다$|이다\\.$|이다$|있다\\.$|있다$|느낀다\\.$|느낀다$|졌다$|졌다\\.$|렸다\\.$|렸다$|났다\\.$|났다$|한다\\.$|한다$|않다$|않다\\.$|했다$|했다\\.$|진다\\.$|진다$|랐다\\.$|랐다\"\n",
    "\n",
    "\n",
    "for idx, summary in enumerate(ko_summary_list):\n",
    "    matched = re.search(pattern, summary)\n",
    "    if matched:\n",
    "        matched_idxs.append(idx)\n",
    "\n",
    "print(len(matched_idxs))\n",
    "\n",
    "new_train = new_train.loc[matched_idxs]\n",
    "new_train = new_train.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 번역문 중 발화자가 제대로 표시되지 않는 혹은 비어있는 값 제거"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_talking_sequence2(x):\n",
    "    pattern = '[a-zA-Z\\'\\s\\-,\\._가-힣]+:'\n",
    "    talking_sequence = []\n",
    "    for talk in x:\n",
    "        matched = re.search(pattern, talk)\n",
    "        \n",
    "        if matched:\n",
    "            person = matched.group(0)\n",
    "        else:\n",
    "            person = \"#ErrorMatch#:\"\n",
    "        \n",
    "        person = person[:-1]\n",
    "        talking_sequence.append(person)\n",
    "        \n",
    "    return talking_sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 발화자 순서 저장하기\n",
    "new_train['ko_talking_sequence'] = new_train['ko_dialogue_list'].apply(get_talking_sequence2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 발화자를 추출하는 과정에서 오류가 발생했던 데이터 제거\n",
    "talk_list_list = new_train['ko_talking_sequence'].tolist()\n",
    "idx_list = []\n",
    "for idx, talk_list in enumerate(talk_list_list):\n",
    "    if \"#ErrorMatch#\" in talk_list:\n",
    "        idx_list.append(idx)\n",
    "        \n",
    "new_train = new_train.drop(idx_list).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 DialogSum 데이터셋과 비슷하게 발화자 마스킹"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import OrderedDict\n",
    "\n",
    "# DialogSum 데이터와 비슷하게 사람 마스킹하기\n",
    "ko_dialogue_list = new_train['ko_dialogue_list'].tolist()   # 대화문 리스트\n",
    "ko_talking_sequence = new_train['ko_talking_sequence'].tolist()     # 발화 순서\n",
    "ko_summary = new_train['ko_summary'].tolist() # 요약문 리스트\n",
    "\n",
    "new_dialogue_list = []  # 새롭게 생성할 대화문\n",
    "new_summary_list = []   # 새롭게 생성할 요약문\n",
    "\n",
    "for i in range(0, len(ko_dialogue_list)):\n",
    "    temp_dialogue_list = []\n",
    "    \n",
    "    person_list = list(OrderedDict.fromkeys(ko_talking_sequence[i]))    # 순서를 유지한체 중복 제거\n",
    "    person_dict = {}    # 발화자 별 마스킹값 할당\n",
    "    for idx, person in enumerate(person_list):\n",
    "        person_dict[person] = f\"#Person{idx+1}#\"\n",
    "    \n",
    "    # print(ko_dialogue_list[i])\n",
    "    # print(ko_talking_sequence[i])\n",
    "    \n",
    "    # person_dict에 할당된 값에 따라서 발화자 순서에 따라서 마스킹 값으로 대체\n",
    "    for j in range(0, len(ko_dialogue_list[i])):\n",
    "        pattern = f\"^{ko_talking_sequence[i][j]}:\"\n",
    "        new_str = re.sub(pattern, person_dict[ko_talking_sequence[i][j]]+\":\", ko_dialogue_list[i][j], count=1)\n",
    "        temp_dialogue_list.append(new_str)\n",
    "    \n",
    "    # person_dict에 할당된 값에 따라서 요약문에 있는 발화자를 마스킹 값으로 대체\n",
    "    new_summary = ko_summary[i]\n",
    "    for k, v in person_dict.items():\n",
    "        # print(k, v)\n",
    "        pattern = f\"{k}\"\n",
    "        new_summary = re.sub(pattern, v, new_summary)\n",
    "        \n",
    "    new_dialogue_list.append(temp_dialogue_list)\n",
    "    new_summary_list.append(new_summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4 데이터 재구성 및 저장"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_train['processed_dialogue_list'] = new_dialogue_list\n",
    "new_train['processed_dialogue'] = new_train['processed_dialogue_list'].apply(lambda x: \"\\n\".join(x))\n",
    "new_train['processed_summary'] = new_summary_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_dataset = new_train[['id', 'ko_summary', 'ko_dialogue', 'processed_dialogue', 'processed_summary']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_dataset.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_dataset.to_csv(\"../data/ko_dataset.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.13 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "vscode": {
   "interpreter": {
    "hash": "d4d1e4263499bec80672ea0156c357c1ee493ec2b1c70f0acce89fc37c4a6abe"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
